---
title: 'Tutorial (week 2) B: text mining'
output:
  html_document: default
  html_notebook: default
toc: yes
toc_depth: 2
---

# Step 0: check and install needed packages. Load the libraries and functions. 

```{r, message=FALSE, warning=FALSE}

packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("xml2")
library("qdap")
library("syuzhet")
library("ggplot2")
library("tm")
library("tidytext")

source("../lib/speechFuncs.R")
```
This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.

we will focus on the inauguration speeches.
find some trends, from 1789 to 2017.

first, load in the data from URL


```{r, message=FALSE, warning=FALSE}
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")
# Get link URLs
# f.speechlinks is a function for extracting links from the list of speeches. 
inaug=f.speechlinks(main.page)
inaug=inaug[-nrow(inaug),] # remove the last line, irrelevant due to error.

```

# Step 2: Using speech metadata posted on <http://www.presidency.ucsb.edu/>, we prepared CSV data sets for the speeches we will scrap. 

```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
#delete the date column due to the formatting error
```

We assemble all scrapped speeches into one list. Note here that we don't have the full text yet, only the links to full text transcripts. 

# Step 3: scrap the texts of speeches from the speech URLs.

```{r}
speech.list=cbind(inaug.list, inaug)
speech.list=speech.list[,-5]
#delete the date column due to formatting error
colnames(speech.list)[colnames(speech.list)=="links"]="Date"
#name the correct date
```

now we have words of every speech, so we could compare them.

```{r}
#abtract the year of the speech
speech.list$year = rep(NA,nrow(speech.list))
for ( i in 1:nrow(speech.list))
      {  n = nchar(speech.list$Date[i])
        speech.list$year[i] = as.numeric(substring(speech.list$Date[i],n-3,n))
}

words.partymean=tapply(speech.list$Words,speech.list$Party,mean)


ggplot(data = speech.list)+
  geom_point(mapping = aes(x=year,y=Words,color=Party))+
  geom_smooth(mapping = aes(x=year,y=Words),method="lm",se=F)
```

```{r}
Demo=speech.list[speech.list$Party=="Democratic",]
Demo=Demo[!is.na(Demo$President),]
Repub=speech.list[speech.list$Party=="Republican",]
Repub=Repub[!is.na(Repub$President),]
Demo.Repub=rbind(Demo,Repub)

ggplot(data = Demo.Repub,mapping = aes(x=year,y=Words,color=Party))+
  geom_point()+
  geom_smooth(se=F)


```


deal with outliers
```{r}
speech.list2=speech.list[-which.max(speech.list$Words),]
speech.list2=speech.list2[-which.min(speech.list$Words),]


words.partymean2=tapply(speech.list2$Words,speech.list2$Party,mean)

ggplot(data = speech.list2)+
  geom_point(mapping = aes(x=year,y=Words,color=Party))+
  geom_smooth(mapping = aes(x=year,y=Words),method="lm",se=F)
```


Based on the list of speeches, we scrap the main text part of the transcript's html page.For reproducibility, we also save our scrapped speeches into our local folder as individual speech files. 

```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/InauguralSpeeches/", 
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```




# Step 4: data Processing --- generate list of sentences

We will use sentences as units of analysis for this project, as sentences are natural languge units for organizing thoughts and ideas. For each extracted sentence, we apply sentiment analysis using [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."

We assign an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as *sentence length* (`word.count`).

```{r, message=FALSE, warning=FALSE}
sentence.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    word.count=word_count(sentences)
    sentence.list=rbind(sentence.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              sent.id=1:length(sentences)
                              )
    )
  }
}
```

Some non-sentences exist in raw data due to erroneous extra end-of sentence marks. 
```{r}
sentence.list=sentence.list[!is.na(sentence.list$word.count),]
```

```{r}
speech.list$word.count=tapply(sentence.list$word.count,sentence.list$year,mean)

ggplot(data = speech.list)+
  geom_point(mapping = aes(x=year,y=word.count,color=Party))+
  geom_smooth(mapping = aes(x=year,y=word.count),se=F,method="lm")


```


```{r,warnings=F}

folder.path="../data/InauguralSpeeches/"
fulltext<-Corpus(DirSource(folder.path))


fulltext<-tm_map(fulltext, stripWhitespace)
fulltext<-tm_map(fulltext, content_transformer(tolower))
fulltext<-tm_map(fulltext, removeWords, stopwords("english"))
fulltext<-tm_map(fulltext, removeWords, character(0))
fulltext<-tm_map(fulltext, removePunctuation)
fulltext<-tm_map(fulltext, stemDocument)
text<-tm_map(fulltext, stripWhitespace)
fulltext<-tm_map(fulltext, removePunctuation)
fulltext<-tm_map(fulltext, removeNumbers)
fulltext<-tm_map(fulltext, PlainTextDocument)



dtm <- DocumentTermMatrix(fulltext) 
dtms <- removeSparseTerms(dtm, 0.1)

```

word frequency
```{r}
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)  
wf <- data.frame(word=names(freq), freq=freq)  
ggplot(subset(wf, freq>300), aes(word, freq))+    
geom_bar(stat="identity")+
theme(axis.text.x=element_text(angle=45, hjust=1))   
```

for some certern word
```{r}
#people
for (i in 1:nrow(speech.list))
     {
       speech.list$people[i]=length(gregexpr("people",speech.list$fulltext[i])[[1]])/speech.list$Words[i]*100
}
ggplot(data=speech.list,aes(x=year,y=people))+
  geom_point(color="purple")+
  geom_smooth(method="lm",se=F)

#govern

for (i in 1:nrow(speech.list))
     {
       speech.list$govern[i]=length(gregexpr("govern",speech.list$fulltext[i])[[1]])/speech.list$Words[i]*100
}
ggplot(data=speech.list,aes(x=year,y=govern))+
  geom_point(color="orange")+
  geom_smooth(method="lm",se=F,color="red")

```

who is "greatest" president?

```{r}

for (i in 1:nrow(speech.list))
     {
       speech.list$great[i]=length(gregexpr("great",speech.list$fulltext[i])[[1]])/speech.list$Words[i]
}
ggplot(data=speech.list,aes(x=year,y=great))+
  geom_bar(stat="identity")

speech.list[which.max(speech.list$great),1:7]
```



